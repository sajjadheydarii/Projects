{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJhomdonjCmg"
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from tools import data_balancing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from feature_processing import feature_preprocessing, feature_processing\n",
    "from tools import data_balancing, multipage, plot_permutation_importance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pACPqOVQHJDL"
   },
   "outputs": [],
   "source": [
    "# Set of functions 1\n",
    "\n",
    "def __plot_correlation_matrix(correlation_matrix):\n",
    "    f = plt.figure(figsize=(10, 8))\n",
    "    plt.matshow(correlation_matrix, fignum=f.number)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    # plt.title('Correlation Matrix', fontsize=16)\n",
    "    # plt.tight_layout()\n",
    "    return\n",
    "\n",
    "\n",
    "def __missing_data_imputation(train_features, valid_features):\n",
    "    \"\"\"MICE = IterativeImputer(random_state=42, max_iter=1000, initial_strategy='median', tol=0.01, n_nearest_features=20)\n",
    "    MICE = MICE.fit(train_features)\n",
    "    train_features = MICE.transform(train_features)\n",
    "    valid_features = MICE.transform(valid_features)\"\"\"\n",
    "    num_neigh = 5\n",
    "    print('Neighbors used for KNN imputer: ' + str(num_neigh))\n",
    "    features_list_fs = train_features.columns\n",
    "    KNN_imputer = KNNImputer(n_neighbors=num_neigh)\n",
    "    KNN_imputer.fit(train_features)\n",
    "    train_features = KNN_imputer.transform(train_features)\n",
    "    valid_features = KNN_imputer.transform(valid_features)\n",
    "\n",
    "    train_features = pd.DataFrame(data=train_features, columns=features_list_fs)\n",
    "    valid_features = pd.DataFrame(data=valid_features, columns=features_list_fs)\n",
    "    return train_features,  valid_features, KNN_imputer\n",
    "\n",
    "\n",
    "def feature_preprocessing(train_features, valid_features):\n",
    "    # a. missing values handling\n",
    "    print('-----------------a. eliminate features with more than 20% of NaNs---------------')\n",
    "\n",
    "    #   a.1. Eliminate subjects with more than a percent of missing features:\n",
    "\n",
    "    #   a.2. Eliminate features with more than a percent of missing values:\n",
    "    print('Eliminating features with more than a % of missing values...')\n",
    "    feat_ini = len(train_features.columns)\n",
    "    th = round((len(train_features) * 20) / 100)  # eliminate variables with more than 50% of the values missing\n",
    "    train_features = train_features.dropna(thresh=th, axis='columns')\n",
    "    valid_features = valid_features[train_features.columns]\n",
    "    print('Features dropped : ' + str(feat_ini - len(train_features.columns)))\n",
    "\n",
    "\n",
    "    # b. Outlier removal\n",
    "    print('-----------------b. Outlier imputation (IQR limit and data imputation)-----------------')\n",
    "    print('Outliers are being treated as NaNs after identification by IQR metrics')\n",
    "    feature_list_fs = train_features.columns\n",
    "    robust_scaler = RobustScaler().fit(train_features)\n",
    "    train_features_rs = robust_scaler.transform(train_features)\n",
    "\n",
    "    # find upper and lower limit to consider outliers based on the IQR\n",
    "    Q1 = np.quantile(train_features_rs, 0.25, axis=0)\n",
    "    Q3 = np.quantile(train_features_rs, 0.75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    lim_up = Q3 + 1.5*IQR\n",
    "    lim_dow = Q1 - 1.5*IQR\n",
    "\n",
    "    # substitute values by: (a) 0.95/0.05 Q (b) nan and then perform value imputation\n",
    "    train_features[train_features_rs < lim_dow] = np.nan\n",
    "    train_features[train_features_rs > lim_up] = np.nan\n",
    "\n",
    "    train_features = pd.DataFrame(data=train_features, columns=feature_list_fs)\n",
    "\n",
    "    print('-----------------c. Missing values imputation----------')\n",
    "    #   a.3. Replace nan values with median\n",
    "    print('Performing NaN imputation with KNN...')\n",
    "    \"\"\"medians = train_features.median()\n",
    "    train_features = train_features.fillna(medians)\n",
    "    valid_features = valid_features.fillna(medians)\"\"\"\n",
    "    train_features, valid_features, KNN_imputer = __missing_data_imputation(train_features, valid_features)\n",
    "\n",
    "    \"\"\"print('Replacing NaN values with MICE...')\n",
    "    MICE = IterativeImputer(random_state=42)\n",
    "    MICE = MICE.fit(train_features)\n",
    "    train_features = MICE.transform(train_features)\n",
    "    valid_features = MICE.transform(valid_features)\"\"\"\n",
    "\n",
    "    # c. scaling/standarization\n",
    "    print('-----------------d. scaling/standarization----------')\n",
    "    print('Applying robust scaler...')\n",
    "    feature_list_fs = train_features.columns\n",
    "    robust_scaler = RobustScaler().fit(train_features)\n",
    "    train_features = robust_scaler.transform(train_features)\n",
    "    valid_features = robust_scaler.transform(valid_features)\n",
    "    train_features = pd.DataFrame(data=train_features, columns=feature_list_fs)\n",
    "    valid_features = pd.DataFrame(data=valid_features, columns=feature_list_fs)\n",
    "\n",
    "    return train_features, valid_features, robust_scaler, KNN_imputer\n",
    "\n",
    "\n",
    "def __feature_selection(train_features, train_labels):\n",
    "    print('-----------------d. feature selection--------------')\n",
    "    print('i- Eliminating features with less than 0.01 variance. ')\n",
    "    #   i.   remove constant variables\n",
    "    feature_list_fs = train_features.columns\n",
    "    vt = VarianceThreshold(threshold=0.01)  # eliminate constant features - Check\n",
    "    temp = vt.fit_transform(train_features)\n",
    "    mask = vt.get_support()\n",
    "    train_features = train_features.loc[:, mask]\n",
    "    print('Features dropped : ' + str(len(feature_list_fs) - len(train_features.columns)))\n",
    "\n",
    "    #  ii. remove correlated features\n",
    "    print('ii- Eliminating correlated features (>0.8). ')\n",
    "    # a. visualize correlation matrix\n",
    "    feature_list_fs = train_features.columns\n",
    "    cor_matrix = train_features.corr().abs()\n",
    "    __plot_correlation_matrix(cor_matrix)\n",
    "\n",
    "    # b. drop correlated features\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = train_features.corr()\n",
    "    median_corr = (correlation_matrix.abs()).median(axis=0)\n",
    "\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.80:\n",
    "                if median_corr[i] < median_corr[j]:\n",
    "                    colname = correlation_matrix.columns[j]\n",
    "                else:\n",
    "                    colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "    train_features.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "    # see how it drops correlated / drop teh one that had a lower correlation w.r to the other feat\n",
    "    print('Features dropped: ' + str(len(correlated_features)))\n",
    "\n",
    "    # c. plot correlation matrix with remaining features\n",
    "    cor_matrix = train_features.corr().abs()\n",
    "    feature_list_fs = list(train_features.columns)\n",
    "    __plot_correlation_matrix(cor_matrix)\n",
    "\n",
    "    # iii. recursive feature selection\n",
    "    print('iii- Recurrent feature selection CV. ')\n",
    "    feature_list_fs = train_features.columns\n",
    "    th_select = 1\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    min_features_to_select = 1\n",
    "    cv_num = 3\n",
    "    train_features_sb, train_labels_sb = data_balancing(train_features, train_labels)\n",
    "    feats_RFE_sbs = pd.DataFrame(data=np.zeros([5, len(feature_list_fs)]), columns=feature_list_fs)\n",
    "    for Nsbs in range(len(train_features_sb)):\n",
    "        rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(cv_num), scoring='balanced_accuracy',\n",
    "                      min_features_to_select=min_features_to_select, n_jobs=-1)\n",
    "        rfecv.fit(train_features_sb[Nsbs], train_labels_sb[Nsbs])\n",
    "        feats_RFE_sbs.iloc[Nsbs,  rfecv.support_] = int(1)\n",
    "    feats_RFE_sbs = feats_RFE_sbs.sum(axis=0)\n",
    "    print('Threshold for feature selection: ' + str(th_select))\n",
    "    feature_list_fs = feature_list_fs[np.where(feats_RFE_sbs >= th_select)]\n",
    "    train_features = train_features[feature_list_fs]\n",
    "    print('Features dropped: ' + str(len(feature_list_fs) - len(train_features.columns)))\n",
    "\n",
    "    \"\"\"# iv sequencial features selection (SFFS)\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    sfs = SequentialFeatureSelector(svc, n_features_to_select=None, direction='forward', scoring='balanced_accuracy',\n",
    "                                    cv=StratifiedKFold(5),)\n",
    "    sfs.fit(train_features, train_labels)\n",
    "    train_features = train_features.iloc[:, sfs.get_support()]\"\"\"\n",
    "\n",
    "    print('Final number of features selected : ' + str(len(train_features.columns)))\n",
    "\n",
    "    return train_features\n",
    "\n",
    "\n",
    "def feature_processing(train_features, train_labels, valid_features, feat_select_flag):\n",
    "    \"1. feature preprocessing\"\n",
    "    train_features, valid_features, robust_scaler, KNN_imputer = feature_preprocessing(train_features, valid_features)\n",
    "\n",
    "    \"2. feature selection\"\n",
    "    if feat_select_flag:\n",
    "        print('Performing feature selection')\n",
    "        train_features = __feature_selection(train_features, train_labels)\n",
    "        feature_list_fs = train_features.columns\n",
    "        valid_features = valid_features[feature_list_fs]\n",
    "\n",
    "    return train_features, valid_features\n",
    "\n",
    "\n",
    "def __plot_confusion_matrix(test_labels, test_predicted, labels):\n",
    "    cm = confusion_matrix(test_labels, test_predicted, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['live', 'death'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def __plot_feature_importance(clf_best, feature_list_fs, valid_features, valid_labels, plot_flag):\n",
    "\n",
    "    \"1. Impurity-based importances  (standard sklearn)\"\n",
    "    impu_importance = np.zeros([len(clf_best), len(feature_list_fs)])\n",
    "    for Nsbs in range(len(clf_best)):\n",
    "        impu_importance[Nsbs, :] = clf_best[Nsbs].feature_importances_\n",
    "    impu_std = np.std(impu_importance, axis=0)\n",
    "    impu_importance = np.mean(impu_importance, axis=0)\n",
    "\n",
    "    \"2. permutation importance\"\n",
    "    perm_importance = np.zeros([len(clf_best), len(feature_list_fs)])\n",
    "    for Nsbs in range(len(clf_best)):\n",
    "        perm_importance_temp = permutation_importance(clf_best[Nsbs], valid_features, valid_labels, n_jobs=-1,\n",
    "                                                      n_repeats=5, random_state=42)\n",
    "        perm_importance[Nsbs, :] = perm_importance_temp.importances_mean\n",
    "\n",
    "    perm_std = np.std(perm_importance, axis=0)\n",
    "    perm_importance = np.mean(perm_importance, axis=0)\n",
    "\n",
    "    importances = np.array([impu_importance, impu_std, perm_importance, perm_std])\n",
    "    feat_importance = pd.DataFrame(data=importances, columns=feature_list_fs, index=['impu_importance', 'impu_std',\n",
    "                                                                                     'perm_importance', 'perm_std'])\n",
    "\n",
    "    if plot_flag:\n",
    "        \"a. impurity based importances\"\n",
    "        sorted_idx = impu_importance.argsort()\n",
    "        clf_importances = pd.Series(impu_importance[sorted_idx[::-1]], index=feature_list_fs[sorted_idx[::-1]])\n",
    "\n",
    "        if len(feature_list_fs) >= 60:\n",
    "            font_size = 6\n",
    "        elif len(feature_list_fs) < 60 & len(feature_list_fs) > 40:\n",
    "            font_size = 8\n",
    "        elif len(feature_list_fs) <= 40:\n",
    "            font_size = 10\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xticks(fontsize=font_size)\n",
    "        plt.yticks(fontsize=font_size)\n",
    "        clf_importances.plot.bar(yerr=impu_std, ax=ax)\n",
    "        ax.set_title(\"Feature impu_importance using MDI\")\n",
    "        ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \"b. permutation importance\"\n",
    "        sorted_idx = perm_importance.argsort()\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xticks(fontsize=font_size)\n",
    "        plt.yticks(fontsize=font_size)\n",
    "        plt.barh(feature_list_fs[sorted_idx], perm_importance[sorted_idx], xerr=perm_std[sorted_idx])\n",
    "        ax.set_title(\"Permutation Importance\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\"\"3. feature importance with SHAP values\"\n",
    "    explainer = shap.TreeExplainer(clf_best)\n",
    "    shap_values = explainer.shap_values(valid_features)\n",
    "    shap.summary_plot(shap_values, valid_features, plot_type=\"bar\")\n",
    "    shap.summary_plot(shap_values, valid_features)\"\"\"\n",
    "\n",
    "    \"------------------------------------------------------\"\n",
    "    \"\"\"# Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(impu_importance[:, sorted_idx])\n",
    "    feature_names = np.array(feature_list_fs)\n",
    "    feature_names = feature_names[sorted_idx]\n",
    "    feature_names = np.tile([feature_names], len(feature_importance)).squeeze()\n",
    "    feature_importance = np.reshape(feature_importance, len(feature_names)) #\n",
    "\n",
    "    feature_importance = feature_importance[::-1]\n",
    "    feature_names = feature_names[::-1]\n",
    "\n",
    "    # Create a DataFrame using a Dictionary\n",
    "    data = {'feature_names': feature_names, 'feature_importance': feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the DataFrame in order decreasing feature importance\n",
    "    # fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n",
    "\n",
    "    # Define size of bar plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.xticks(fontsize=font_size)\n",
    "    plt.yticks(fontsize=font_size)\n",
    "    # Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'], errwidth=1)\n",
    "    # Add chart labels\n",
    "    plt.title('Feature importance')\n",
    "    # plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    plt.tight_layout()\"\"\"\n",
    "    return feat_importance\n",
    "\n",
    "\n",
    "def __model_evaluation(valid_predict, valid_labels):\n",
    "    # predictions = model.predict(valid_features_df)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(valid_labels, valid_predict).ravel()\n",
    "\n",
    "    model_metrics_1 = pd.DataFrame(\n",
    "        {'acc_bal': [metrics.balanced_accuracy_score(valid_labels, valid_predict)],\n",
    "         'se': [metrics.recall_score(valid_labels, valid_predict, average='binary', pos_label=1)],\n",
    "         'sp': [tn / (tn + fp)],\n",
    "         'PPV': [metrics.precision_score(valid_labels, valid_predict, average='binary', pos_label=1)],\n",
    "         'f1_score': [metrics.f1_score(valid_labels, valid_predict, average='binary', pos_label=1)]})\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix((valid_labels-1) * -1, (valid_predict - 1) * -1).ravel()\n",
    "    model_metrics_0 = pd.DataFrame(\n",
    "        {'acc_bal': [metrics.balanced_accuracy_score(valid_labels, valid_predict)],\n",
    "         'se': [metrics.recall_score(valid_labels, valid_predict, average='binary', pos_label=0)],\n",
    "         'sp': [tn / (tn + fp)],\n",
    "         'PPV': [metrics.precision_score(valid_labels, valid_predict, average='binary', pos_label=0)],\n",
    "         'f1_score': [metrics.f1_score(valid_labels, valid_predict, average='binary', pos_label=0)]})\n",
    "\n",
    "    print(metrics.classification_report(valid_labels, valid_predict))\n",
    "    __plot_confusion_matrix(valid_labels, valid_predict, [0, 1])\n",
    "    return model_metrics_1, model_metrics_0\n",
    "\n",
    "\n",
    "def __model_hyperparam_tunning(train_features_sb, train_labels_sb):\n",
    "    # 1. Define the values for each hyperparameter\n",
    "    parameters = {\"n_estimators\": (10, 1000),\n",
    "                  \"max_depth\": (1, 150),\n",
    "                  \"min_samples_split\": (2, 10),\n",
    "                  \"class_weight\": ['balanced_subsample']}\n",
    "\n",
    "    # 2. Crossvalidate model to obtain the best fit\n",
    "    # create classifier grid: define classifier, hyperparam values and scoring metric\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    clf = BayesSearchCV(estimator=RandomForestClassifier(random_state=42), search_spaces=parameters, cv=cv, scoring='f1_macro')\n",
    "    clf.fit(train_features_sb, train_labels_sb)\n",
    "\n",
    "    # print results for each combination of parameters\n",
    "    for param, score in zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score']):\n",
    "        print(param, score)\n",
    "\n",
    "    # display best combination of parameters\n",
    "    print('Best combination of hyperparameters: ' + str(clf.best_params_))\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def __threshold_optimization(predict_prob, valid_labels):\n",
    "    # predict probabilities\n",
    "    # predict_proba = clf_best.predict_proba(valid_features)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    # predict_proba = predict_proba[:, 1]\n",
    "    # calculate roc curves\n",
    "    precision, recall, thresholds = precision_recall_curve(valid_labels, predict_prob)\n",
    "    # convert to f score\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    fscore = np.nan_to_num(fscore)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "\n",
    "    threshold_F1score = thresholds[ix]\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    fig, ax = plt.subplots()\n",
    "    no_skill = len(valid_labels[valid_labels == 1]) / len(valid_labels)\n",
    "    pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(recall, precision, marker='.', label='model')\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    return threshold_F1score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multipage(filename, figs=None, dpi=200):\n",
    "    pp = PdfPages(filename)\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for fig in figs:\n",
    "        fig.savefig(pp, format='pdf')\n",
    "    pp.close()\n",
    "\n",
    "\n",
    "def data_balancing(train_features, train_labels):\n",
    "    print('Total patients: ' + str(len(train_features)) + '(Class 1: ' + str(\n",
    "        sum(train_labels == 1)) + ',  Class 0: ' + str(sum(train_labels == 0)), ')')\n",
    "    num_sbs = 5\n",
    "    random.seed(a=42, version=2)\n",
    "    rand_sb = random.sample(range(0, sum(train_labels == 0)), sum(train_labels == 0))\n",
    "    subset_idx_C0 = np.where(train_labels == 0)[0]\n",
    "    subset_idx_C1 = np.where(train_labels == 1)[0]\n",
    "    train_features_sb = list()\n",
    "    train_labels_sb = list()\n",
    "    step_sb = int(sum(train_labels == 0)/ num_sbs)\n",
    "    ini = 0\n",
    "    for Nsbs in range(num_sbs):\n",
    "        fini =  ini + step_sb\n",
    "        if  Nsbs==num_sbs:\n",
    "            fini = sum(train_labels == 0)\n",
    "\n",
    "        # choose the same number of 0 and 1 class to introduce into the subset\n",
    "        train_features_sb.append(pd.concat([train_features.iloc[subset_idx_C0[rand_sb[ini:fini]], :],\n",
    "                                       train_features.iloc[subset_idx_C1]]))\n",
    "\n",
    "        train_labels_sb.append(np.concatenate([train_labels[subset_idx_C0[rand_sb[ini:fini]]],\n",
    "                                                train_labels[subset_idx_C1]]))\n",
    "\n",
    "        # mix 0 and 1 class randomly inside the subset\n",
    "        rand_sb_2 = random.sample(range(0, len(train_features_sb[Nsbs])), len(train_features_sb[Nsbs]))\n",
    "        train_features_sb[Nsbs] = train_features_sb[Nsbs].iloc[rand_sb_2, :]\n",
    "        train_labels_sb[Nsbs] = train_labels_sb[Nsbs][rand_sb_2]\n",
    "        ini = fini + 1\n",
    "\n",
    "        print('S' + str(Nsbs+1) + ' - Total patients: ' + str(len(train_features_sb[Nsbs])) + '(Class 1: ' + str(\n",
    "            sum(train_labels_sb[Nsbs] == 1)) + ',  Class 0: ' + str(sum(train_labels_sb[Nsbs] == 0)), ')')\n",
    "\n",
    "    return train_features_sb, train_labels_sb\n",
    "\n",
    "\n",
    "def plot_permutation_importance(model_metrics_1, per_importance, feature_list_fs):\n",
    "    # Study the permutation importance for final feature selection?\n",
    "    weights_median = model_metrics_1['acc_bal'] / model_metrics_1['acc_bal'].max(axis=0)\n",
    "    per_importance_median = per_importance.median(axis=0)\n",
    "    # per_importance_wmean = (per_importance*weights_median).sum(axis=0)/weights_median.sum()\n",
    "    per_importance_std = per_importance.std(axis=0)\n",
    "    sorted_idx = per_importance_median.argsort()\n",
    "    fig, ax = plt.subplots()\n",
    "    font_size = 6\n",
    "    plt.xticks(fontsize=font_size)\n",
    "    plt.yticks(fontsize=font_size)\n",
    "    plt.barh(feature_list_fs[sorted_idx], per_importance_median[sorted_idx], xerr=per_importance_std[sorted_idx])\n",
    "    ax.set_title(\"Permutation Importance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4LQLYgEi08h"
   },
   "outputs": [],
   "source": [
    "# Set of functions 2\n",
    "\n",
    "\n",
    "def __model_training(train_features, train_labels, valid_features, valid_labels, hyper_tun_flag):\n",
    "    \"\"\"model = RandomForestClassifier(class_weight='balanced_subsample', random_state=42, )\n",
    "    model.fit(train_features_sb, train_labels_sb)\"\"\"\n",
    "    print('Model training:')\n",
    "\n",
    "    # 1. Data balancing\n",
    "    print('1. Balancing classes...')\n",
    "    train_features_sb, train_labels_sb = data_balancing(train_features, train_labels)\n",
    "    # create empty array that will contain the probabilities predicted with each of the models of the ensemble\n",
    "    valid_predict_prob = np.zeros([len(valid_labels), 2, len(train_features_sb)])\n",
    "    clf_best = list()\n",
    "    for Nsbs in range(len(train_features_sb)):\n",
    "        # 2. Hyperparameter tuning using bayesian optimization\n",
    "        if hyper_tun_flag:\n",
    "            print('E' + str(Nsbs) + ' - 2. Tuning hyperparameters using Bayesian optimization...')\n",
    "            clf = __model_hyperparam_tunning(train_features_sb[Nsbs], train_labels_sb[Nsbs])\n",
    "            clf_params = clf.best_params_\n",
    "        else:\n",
    "            print('E' + str(Nsbs) +  ' - 2. Using preset hyperparameters* ...')\n",
    "            clf_params = {\"n_estimators\": 1000,\n",
    "                          \"max_depth\": 101,\n",
    "                          \"min_samples_split\": 3,\n",
    "                          \"class_weight\": 'balanced_subsample'}\n",
    "\n",
    "        # 3. Model training using the best parameters\n",
    "        print('E' + str(Nsbs) +  ' - 3. Training the model with the best parameters...')\n",
    "        # clf_best = RandomForestClassifier(**clf.best_params_)\n",
    "        clf_best.append(RandomForestClassifier(**clf_params))\n",
    "        clf_best[Nsbs].fit(train_features_sb[Nsbs], train_labels_sb[Nsbs])\n",
    "\n",
    "        # 4. predict probabilities of the validation set for each model of the ensemble\n",
    "        valid_predict_prob[:, :, Nsbs] = clf_best[Nsbs].predict_proba(valid_features)\n",
    "\n",
    "    # 5. Threshold optimization\n",
    "    # Average the probabilities of the different ensembles\n",
    "    valid_predict_prob = np.mean(valid_predict_prob, axis=2)\n",
    "    threshold_F1score = __threshold_optimization(valid_predict_prob[:, 1], valid_labels)\n",
    "    valid_predict = np.zeros(len(valid_predict_prob))\n",
    "    positive_out = np.where(valid_predict_prob[:, 1] > threshold_F1score)\n",
    "    valid_predict[positive_out[0]] = int(1)\n",
    "\n",
    "    # 5. Model evaluation\n",
    "    print('4.Evaluating the model with the best parameters')\n",
    "    feature_list_fs = train_features.columns\n",
    "    valid_features = valid_features[feature_list_fs]\n",
    "    model_metrics_1, model_metrics_0 = __model_evaluation(valid_predict, valid_labels)\n",
    "\n",
    "    # 5. Plot feature importance\n",
    "    plot_flag = False\n",
    "    feat_importance = __plot_feature_importance(clf_best, feature_list_fs, valid_features, valid_labels, plot_flag)\n",
    "    return clf_best, threshold_F1score, model_metrics_1, model_metrics_0, feat_importance\n",
    "\n",
    "\n",
    "def crossvalidation_training_pipeline(kfolds_n, train_features_tv, train_labels_tv, hyper_tun_flag, feat_select_flag,\n",
    "                                      perm_import_flag, saving_path):\n",
    "    NKf = 0\n",
    "    skf = StratifiedKFold(n_splits=kfolds_n, shuffle=True, random_state=42)  # random_state=42\n",
    "    threshold_F1score = np.zeros(kfolds_n)\n",
    "    feature_list = train_features_tv.columns\n",
    "    feature_selection_df = pd.DataFrame(data=np.zeros([len(feature_list), kfolds_n + 1]), index=feature_list)\n",
    "    model_metrics_1 = pd.DataFrame(data=np.zeros([kfolds_n, 5]), columns=['acc_bal', 'se',\n",
    "                                                                          'sp', 'PPV', 'f1_score'])\n",
    "\n",
    "    model_metrics_0 = pd.DataFrame(data=np.zeros([kfolds_n, 5]), columns=['acc_bal', 'se',\n",
    "                                                                          'sp', 'PPV', 'f1_score'])\n",
    "\n",
    "    per_importance = pd.DataFrame(data=np.zeros([kfolds_n, len(feature_list)]), columns=feature_list)\n",
    "    for train_index, valid_index in skf.split(train_features_tv, train_labels_tv):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", valid_index)\n",
    "        train_features, valid_features = train_features_tv.iloc[train_index, :], train_features_tv.iloc[valid_index, :]\n",
    "        train_labels, valid_labels = train_labels_tv[train_index], train_labels_tv[valid_index]\n",
    "\n",
    "        # Inside cross-validation\n",
    "        print('Training Features Shape:', train_features.shape, 'Training Labels Shape:', train_labels.shape)\n",
    "        print('Validation Features Shape:', valid_features.shape, 'Validation Labels Shape:', valid_labels.shape)\n",
    "\n",
    "        \"b. feature selection\"\n",
    "        train_features, valid_features = feature_processing(train_features, train_labels, valid_features,\n",
    "                                                                  feat_select_flag)\n",
    "        feature_list_fs = list(train_features.columns)\n",
    "        feature_selection_df.loc[feature_list_fs, NKf] = 1\n",
    "\n",
    "        \"c. Model training\"\n",
    "        model, threshold_F1score_temp, model_metrics_1_temp, model_metrics_0_temp, feat_importance = \\\n",
    "            __model_training(train_features, train_labels, valid_features, valid_labels, hyper_tun_flag)\n",
    "\n",
    "        model_metrics_1.iloc[NKf, :] = model_metrics_1_temp\n",
    "        model_metrics_0.iloc[NKf, :] = model_metrics_0_temp\n",
    "        threshold_F1score[NKf] = threshold_F1score_temp\n",
    "        if perm_import_flag:\n",
    "            per_importance.iloc[NKf, :] = feat_importance.loc['perm_importance']\n",
    "\n",
    "\n",
    "        \"d. Saving parameters\"\n",
    "        saving_path_temp = os.path.join(saving_path, 'KFold_' + str(NKf + 1))\n",
    "        if not os.path.exists(saving_path_temp):\n",
    "            os.makedirs(saving_path_temp)\n",
    "\n",
    "        # 1. save features selected\n",
    "        pickle.dump(feature_list_fs, open(os.path.join(saving_path_temp, 'features_selected.pkl'), \"wb\"))\n",
    "        # 2. save trained model\n",
    "        pickle.dump(model, open(os.path.join(saving_path_temp, 'random_forest.sav'), \"wb\"))\n",
    "        # 3. save threshold\n",
    "        pickle.dump(threshold_F1score_temp, open(os.path.join(saving_path_temp, 'threshold_F1score.pkl'), \"wb\"))\n",
    "        # 4. save figures\n",
    "        multipage(os.path.join(saving_path_temp, 'figures_K' + str(NKf + 1) + '.pdf'))\n",
    "        plt.close('all')\n",
    "        # 5. current model metrics\n",
    "        pickle.dump(model_metrics_1_temp, open(os.path.join(saving_path_temp, 'model_metrics_1.pkl'), \"wb\"))\n",
    "        # 6. save current train-validation split\n",
    "        pickle.dump(train_index, open(os.path.join(saving_path_temp, 'train_index.pkl'), \"wb\"))\n",
    "        pickle.dump(valid_index, open(os.path.join(saving_path_temp, 'valid_index.pkl'), \"wb\"))\n",
    "        # 7. Save features importance\n",
    "        feat_importance.to_csv((os.path.join(saving_path_temp, 'feature_importance.csv')))\n",
    "        NKf = NKf + 1\n",
    "\n",
    "    features_Kfold = feature_selection_df.sum(axis=1)\n",
    "    features_Kfold.hist()\n",
    "\n",
    "    \"e. median permutation importance\"\n",
    "    if perm_import_flag:\n",
    "        plot_permutation_importance(model_metrics_1, per_importance, feature_list_fs)\n",
    "\n",
    "    \"f. save output of full k-fold\"\n",
    "    saving_path_temp = os.path.join(saving_path, 'all')\n",
    "    if not os.path.exists(saving_path_temp):\n",
    "        os.makedirs(saving_path_temp)\n",
    "\n",
    "    model_metrics_kfold = pd.DataFrame({'class_0_mean': model_metrics_0.mean(), 'class_0_std': model_metrics_0.std(),\n",
    "                                        'class_1_mean': model_metrics_1.mean(), 'class_1_std': model_metrics_1.std()})\n",
    "\n",
    "    model_metrics_1.to_csv((os.path.join(saving_path_temp, 'model_metrics_1.csv')))\n",
    "    model_metrics_0.to_csv((os.path.join(saving_path_temp, 'model_metrics_0.csv')))\n",
    "    model_metrics_kfold.to_csv((os.path.join(saving_path_temp, 'model_metrics_kfold.csv')))\n",
    "    feature_selection_df.to_csv((os.path.join(saving_path_temp, 'features_selected.csv')))\n",
    "    pickle.dump(threshold_F1score, open(os.path.join(saving_path_temp, 'threshold_F1score.pkl'), \"wb\"))\n",
    "\n",
    "    multipage(os.path.join(saving_path_temp, 'figures.pdf'))\n",
    "    plt.close('all')\n",
    "\n",
    "    return feature_selection_df, threshold_F1score, model_metrics_kfold, model_metrics_1, model_metrics_0, per_importance\n",
    "\n",
    "\n",
    "def model_testing(train_features_tv, train_labels_tv, test_features, test_labels, feature_list_fs, threshold_final,\n",
    "                  saving_path):\n",
    "    # feature_list = train_features_tv.columns\n",
    "    # -----------------I. Data preparation for model testing----------------------\n",
    "    # 1. Feature selection\n",
    "    \"Chosen features\"\n",
    "    # feature_list_fs = train_features_sb.columns\n",
    "\n",
    "    \"select only chosen features\"\n",
    "    test_features = test_features[feature_list_fs]\n",
    "    train_features_tv = train_features_tv[feature_list_fs]\n",
    "\n",
    "    # 2. feature processing\n",
    "    train_features_tv, test_features, robust_scaler, KNN_imputer = feature_preprocessing(train_features_tv,\n",
    "                                                                                         test_features)\n",
    "\n",
    "    # 3. Training data balancing\n",
    "    print('1. Balancing classes...')\n",
    "    train_features_sb, train_labels_sb = data_balancing(train_features_tv, train_labels_tv)\n",
    "    # create empty array that will contain the probabilities predicted with each of the models of the ensemble\n",
    "    train_predict_prob = np.zeros([len(train_labels_tv), 2, len(train_features_sb)])\n",
    "    test_predict_prob = np.zeros([len(test_labels), 2, len(train_features_sb)])\n",
    "\n",
    "    clf_final = list()\n",
    "    for Nsbs in range(len(train_features_sb)):\n",
    "        # 2. Hyperparameter tuning using bayesian optimization\n",
    "        print('E' + str(Nsbs) + ' - 2. Using preset hyperparameters* ...')\n",
    "        clf_params = {\"n_estimators\": 1000,\n",
    "                      \"max_depth\": 101,\n",
    "                      \"min_samples_split\": 3,\n",
    "                      \"class_weight\": 'balanced_subsample'}\n",
    "\n",
    "        # 3. Model training using the best parameters\n",
    "        print('E' + str(Nsbs) + ' - 3. Training the model with the best parameters...')\n",
    "        # clf_final = RandomForestClassifier(**clf.best_params_)\n",
    "        clf_final.append(RandomForestClassifier(**clf_params))\n",
    "        clf_final[Nsbs].fit(train_features_sb[Nsbs], train_labels_sb[Nsbs])\n",
    "\n",
    "        # 4. predict probabilities of the validation set for each model of the ensemble\n",
    "        train_predict_prob[:, :, Nsbs] = clf_final[Nsbs].predict_proba(train_features_tv)\n",
    "        test_predict_prob[:, :, Nsbs] = clf_final[Nsbs].predict_proba(test_features)\n",
    "\n",
    "        print('Total patients: ' + str(len(train_features_sb[Nsbs])) + '(Class 1: ' + str(\n",
    "            sum(train_labels_sb[Nsbs] == 1)) +\n",
    "              ',  Class 0: ' + str(sum(train_labels_sb[Nsbs] == 0)), ')')\n",
    "\n",
    "    # -----------------III. Model testing----------------------\n",
    "    # test model\n",
    "    # training score\n",
    "    print('-------------Train---------------')\n",
    "    train_predict_prob = np.mean(train_predict_prob, axis=2)\n",
    "    train_predict = np.zeros(len(train_predict_prob))\n",
    "    positive_out = np.where(train_predict_prob[:, 1] > threshold_final)\n",
    "    train_predict[positive_out[0]] = int(1)\n",
    "    train_metrics_1, train_metrics_0 = __model_evaluation(train_predict, train_labels_tv)\n",
    "\n",
    "    # testing score\n",
    "    print('-------------Test---------------')\n",
    "    test_predict_prob = np.mean(test_predict_prob, axis=2)\n",
    "    test_predict = np.zeros(len(test_predict_prob))\n",
    "    positive_out = np.where(test_predict_prob[:, 1] > threshold_final)\n",
    "    test_predict[positive_out[0]] = int(1)\n",
    "    test_metrics_1, test_metrics_0 = __model_evaluation(test_predict, test_labels)\n",
    "\n",
    "    metrics_model = pd.concat([train_metrics_0, train_metrics_1, test_metrics_0, test_metrics_1])\n",
    "    metrics_model.index = [\"train_0\", \"train_1\", \"test_0\", \"test_1\"]\n",
    "\n",
    "    \"f. Saving test parameters\"\n",
    "    if not os.path.exists(saving_path):\n",
    "        os.makedirs(saving_path)\n",
    "\n",
    "    # 1. save features selected\n",
    "    pickle.dump(feature_list_fs, open(os.path.join(saving_path, 'features_selected.pkl'), \"wb\"))\n",
    "    # 2. save trained model\n",
    "    pickle.dump(clf_final, open(os.path.join(saving_path, 'random_forest_ensemble.sav'), \"wb\"))\n",
    "    # 3. save threshold\n",
    "    pickle.dump(threshold_final, open(os.path.join(saving_path, 'threshold_F1score.pkl'), \"wb\"))\n",
    "    # 4. save figures\n",
    "    multipage(os.path.join(saving_path, 'figures_final.pdf'))\n",
    "    # 5. model metrics\n",
    "    pickle.dump(metrics_model, open(os.path.join(saving_path, 'metrics_model.pkl'), \"wb\"))\n",
    "    # 6. robust scaler\n",
    "    pickle.dump(robust_scaler, open(os.path.join(saving_path, 'robust_scaler.pkl'), \"wb\"))\n",
    "    # 7. KNN imputer\n",
    "    pickle.dump(KNN_imputer, open(os.path.join(saving_path, 'KNN_imputer.pkl'), \"wb\"))\n",
    "\n",
    "    return clf_final, train_metrics_1, train_metrics_0, test_metrics_1, test_metrics_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HStpxd6ZHcRG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from parsel_files import parsel_files_f\n",
    "from model_train_test import model_testing, crossvalidation_training_pipeline\n",
    "\n",
    "\n",
    "saving_path = 'G:\\\\Mi unidad\\\\3_POLIMI\\\\A_Projects\\\\11P_COVIDSQUARE\\\\Analysis\\\\Feature_analysis\\\\output\\\\'\n",
    "\" I. load data from csv file\"\n",
    "features = pd.read_csv('data.csv')\n",
    "\n",
    "labels = np.array(features['Label'])\n",
    "features = features.set_index('Record_Name')\n",
    "\"II. Model ------------------------------------------------------------------------------------------------------------\"\n",
    "\n",
    "\"a. Train - Test split\"\n",
    "# elim = [33, 99, 272, 367, 437, 489, 671, 747, 772, 854, 1029, 1121] no data on glasgow and bravo reports\n",
    "# Read test IDs.\n",
    "test_IDs = list(pd.read_csv('validation_records.csv').squeeze().sort_values())\n",
    "# Select features and labels from the corresponding test IDs\n",
    "test_features = features.loc[test_IDs]\n",
    "test_labels = np.array(test_features['Label'])\n",
    "test_features = test_features.drop(columns=['Label'])\n",
    "print('Testing Features Shape:', test_features.shape, 'Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "# Eliminate test patients from the training and validation dataset \n",
    "train_features_tv = features.drop(features.loc[test_IDs].index)\n",
    "train_labels_tv = np.array(train_features_tv['Label'])\n",
    "train_features_tv = train_features_tv.drop(columns=['Label'])\n",
    "\n",
    "\n",
    "\"b. Crossvalidating for feature selection and cv scores\"\n",
    "# use only training: split train-validation\n",
    "run_folder = 'P4'\n",
    "cv_type = ['RFE', 'PI', 'final']\n",
    "hyper_tun_flag = [False, False, False]\n",
    "feat_select_flag = [True, False, False]\n",
    "perm_import_flag = [False, True, True]\n",
    "\n",
    "Threshold_RFE = 3\n",
    "Threshold_PI = 0\n",
    "kfolds_n = 10\n",
    "\n",
    "# still there is an unsolved error in the crossvalidation function\n",
    "for cv_t in range(len(cv_type)):\n",
    "    print('----------------CV:  ' + cv_type[cv_t] + '----------------')\n",
    "    \"b. cross-validation training with feature selection\"\n",
    "    saving_path_temp = os.path.join(saving_path, run_folder, 'CV', cv_type[cv_t])\n",
    "\n",
    "    feature_selection_df, threshold_F1score, model_metrics_kfold, model_metrics_1, model_metrics_0, per_importance = \\\n",
    "        crossvalidation_training_pipeline(kfolds_n, train_features_tv, train_labels_tv, hyper_tun_flag[cv_t],\n",
    "                                          feat_select_flag[cv_t], perm_import_flag[cv_t],  saving_path_temp)\n",
    "\n",
    "    if cv_t == 0:\n",
    "        # Decide set of features\n",
    "        features_Kfold = feature_selection_df.sum(axis=1)\n",
    "        features_selection = features_Kfold.iloc[np.where(features_Kfold >= Threshold_RFE)]\n",
    "        feature_list_fs = features_selection.index\n",
    "        train_features_tv = train_features_tv[feature_list_fs]\n",
    "\n",
    "    elif cv_t == 1:\n",
    "        # Decide set of features\n",
    "        per_importance_median = per_importance.median(axis=0)\n",
    "        features_selection = per_importance_median[per_importance_median > Threshold_PI]\n",
    "        feature_list_fs = features_selection.index\n",
    "        train_features_tv = train_features_tv[feature_list_fs]\n",
    "\n",
    "\n",
    "\"d. Model testing\"\n",
    "# Decide final threshold\n",
    "threshold_final = np.median(threshold_F1score)\n",
    "print('Testing model using Threshold for number of features in Kfold: ' + str(Threshold_RFE) +\n",
    "      'and Threshold f1score: ' + str(threshold_final))\n",
    "\n",
    "saving_path = os.path.join(saving_path, run_folder, 'test')\n",
    "clf_final, train_metrics_1, train_metrics_0, test_metrics_1, test_metrics_0 = \\\n",
    "    model_testing(train_features_tv, train_labels_tv, test_features, test_labels, feature_list_fs, threshold_final,\n",
    "                  saving_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COVID2_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
